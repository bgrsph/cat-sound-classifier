{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142b20be",
   "metadata": {},
   "source": [
    "# Model Diagnostics\n",
    "\n",
    "Comprehensive analysis of model performance, errors, and cross-validation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b840be",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/training_results.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m categories = \u001b[38;5;28mlist\u001b[39m(label_to_idx.keys())\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load training results (saved from train.ipynb)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRESULTS_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_results.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     21\u001b[39m     training_results = pickle.load(f)\n\u001b[32m     23\u001b[39m history = training_results[\u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/cat-sound-classifier/venv/lib/python3.14/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../results/training_results.pkl'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data/interim\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "\n",
    "# Load metadata\n",
    "with open(DATA_DIR / \"metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "idx_to_label = metadata[\"idx_to_label\"]\n",
    "label_to_idx = metadata[\"label_to_idx\"]\n",
    "categories = list(label_to_idx.keys())\n",
    "\n",
    "# Load training results (saved from train.ipynb)\n",
    "with open(RESULTS_DIR / \"training_results.pkl\", \"rb\") as f:\n",
    "    training_results = pickle.load(f)\n",
    "\n",
    "history = training_results[\"history\"]\n",
    "cv_results = training_results[\"cv_results\"]\n",
    "results = training_results[\"test_results\"]\n",
    "n_classes = training_results[\"n_classes\"]\n",
    "\n",
    "labels = [idx_to_label[i] for i in range(n_classes)]\n",
    "\n",
    "print(f\"Categories: {categories}\")\n",
    "print(f\"Loaded results from {RESULTS_DIR / 'training_results.pkl'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for spectrogram visualization\n",
    "X_test = np.load(DATA_DIR / \"X_test.npy\")\n",
    "y_test = np.load(DATA_DIR / \"y_test.npy\")\n",
    "\n",
    "print(f\"Test data loaded: {X_test.shape}\")\n",
    "print(f\"Training history: {len(history['train_loss'])} epochs\")\n",
    "print(f\"CV results: {len(cv_results['fold_results'])} folds\")\n",
    "print(f\"Test accuracy: {results['accuracy']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492aea68",
   "metadata": {},
   "source": [
    "## 1. Cross-Validation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af902a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV results loaded from training_results.pkl\n",
    "print(f\"CV Accuracy: {cv_results['mean_acc']:.1%} ¬± {cv_results['std_acc']:.1%}\")\n",
    "fold_accs = [f\"{r['best_val_acc']:.1%}\" for r in cv_results['fold_results']]\n",
    "print(f\"Folds: {fold_accs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation bar plot with error bars\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Individual fold accuracies\n",
    "folds = [r[\"fold\"] for r in cv_results[\"fold_results\"]]\n",
    "accs = cv_results[\"accuracies\"]\n",
    "colors = ['#3498db' if a < cv_results['mean_acc'] else '#2ecc71' for a in accs]\n",
    "\n",
    "bars = axes[0].bar(folds, accs, color=colors, edgecolor='white', linewidth=2)\n",
    "axes[0].axhline(cv_results[\"mean_acc\"], color='#e74c3c', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {cv_results[\"mean_acc\"]:.3f}')\n",
    "axes[0].fill_between([0.5, 5.5], \n",
    "                      cv_results[\"mean_acc\"] - cv_results[\"std_acc\"],\n",
    "                      cv_results[\"mean_acc\"] + cv_results[\"std_acc\"],\n",
    "                      color='#e74c3c', alpha=0.2, label=f'¬±1 Std: {cv_results[\"std_acc\"]:.3f}')\n",
    "axes[0].set_xlabel(\"Fold\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Validation Accuracy\", fontsize=12)\n",
    "axes[0].set_title(\"Per-Fold Accuracy\", fontsize=14)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_xticks(folds)\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accs):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{acc:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: Summary with error bar\n",
    "axes[1].bar(['5-Fold CV'], [cv_results[\"mean_acc\"]], \n",
    "            yerr=[cv_results[\"std_acc\"]], capsize=10, color='#3498db',\n",
    "            edgecolor='white', linewidth=2, error_kw={'linewidth': 2})\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=12)\n",
    "axes[1].set_title(\"Cross-Validation Summary\", fontsize=14)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].text(0, cv_results[\"mean_acc\"] + cv_results[\"std_acc\"] + 0.05,\n",
    "             f'{cv_results[\"mean_acc\"]:.1%} ¬± {cv_results[\"std_acc\"]:.1%}',\n",
    "             ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470811e",
   "metadata": {},
   "source": [
    "### Training Curves per Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd32a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves for each fold\n",
    "n_folds = len(cv_results[\"fold_results\"])\n",
    "fig, axes = plt.subplots(n_folds, 2, figsize=(14, 3.5 * n_folds))\n",
    "\n",
    "for i, fold_result in enumerate(cv_results[\"fold_results\"]):\n",
    "    history = fold_result[\"history\"]\n",
    "    fold_num = fold_result[\"fold\"]\n",
    "    best_acc = fold_result[\"best_val_acc\"]\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[i, 0].plot(history[\"train_loss\"], label=\"Train\", linewidth=2)\n",
    "    axes[i, 0].plot(history[\"val_loss\"], label=\"Val\", linewidth=2)\n",
    "    axes[i, 0].set_xlabel(\"Epoch\")\n",
    "    axes[i, 0].set_ylabel(\"Loss\")\n",
    "    axes[i, 0].set_title(f\"Fold {fold_num} - Loss\")\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[i, 1].plot(history[\"train_acc\"], label=\"Train\", linewidth=2)\n",
    "    axes[i, 1].plot(history[\"val_acc\"], label=\"Val\", linewidth=2)\n",
    "    best_epoch = np.argmax(history[\"val_acc\"])\n",
    "    axes[i, 1].axvline(best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best epoch: {best_epoch+1}')\n",
    "    axes[i, 1].set_xlabel(\"Epoch\")\n",
    "    axes[i, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[i, 1].set_title(f\"Fold {fold_num} - Accuracy (Best: {best_acc:.1%})\")\n",
    "    axes[i, 1].legend()\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "    axes[i, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d75c6b",
   "metadata": {},
   "source": [
    "### Single Training Run Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd13719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves from single training run\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history[\"train_loss\"], label=\"Train\", linewidth=2)\n",
    "ax1.plot(history[\"val_loss\"], label=\"Val\", linewidth=2)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history[\"train_acc\"], label=\"Train\", linewidth=2)\n",
    "ax2.plot(history[\"val_acc\"], label=\"Val\", linewidth=2)\n",
    "best_epoch = np.argmax(history[\"val_acc\"])\n",
    "ax2.axvline(best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best: epoch {best_epoch+1}')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_title(f\"Training Accuracy (Best Val: {max(history['val_acc']):.1%})\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446135bb",
   "metadata": {},
   "source": [
    "## 2. Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea723a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results loaded from training_results.pkl\n",
    "print(f\"Test Accuracy: {results['accuracy']:.1%}\")\n",
    "print(f\"Test Loss: {results['loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112a7db",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(results[\"labels\"], results[\"predictions\"])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "disp1 = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "disp1.plot(ax=axes[0], cmap=\"Blues\", xticks_rotation=45)\n",
    "axes[0].set_title(f\"Confusion Matrix - Counts\\n(Test Accuracy: {results['accuracy']:.1%})\")\n",
    "\n",
    "# Normalized (row-wise = recall per class)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_norm = np.nan_to_num(cm_norm)  # Handle division by zero\n",
    "disp2 = ConfusionMatrixDisplay(cm_norm, display_labels=labels)\n",
    "disp2.plot(ax=axes[1], cmap=\"Blues\", xticks_rotation=45, values_format='.2f')\n",
    "axes[1].set_title(\"Confusion Matrix - Normalized (Recall)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16d0dd",
   "metadata": {},
   "source": [
    "### Per-Class Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_true_labels = [idx_to_label[i] for i in results[\"labels\"]]\n",
    "y_pred_labels = [idx_to_label[i] for i in results[\"predictions\"]]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, labels=labels, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    results[\"labels\"], results[\"predictions\"], labels=range(n_classes), zero_division=0\n",
    ")\n",
    "\n",
    "x = np.arange(n_classes)\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars1 = ax.bar(x - width, precision, width, label='Precision', color='#3498db')\n",
    "bars2 = ax.bar(x, recall, width, label='Recall', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Category', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a38333",
   "metadata": {},
   "source": [
    "## 3. Error Analysis\n",
    "\n",
    "Examine misclassified samples to understand model failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified_idx = np.where(results[\"predictions\"] != results[\"labels\"])[0]\n",
    "correct_idx = np.where(results[\"predictions\"] == results[\"labels\"])[0]\n",
    "\n",
    "print(f\"Correct: {len(correct_idx)}/{len(results['labels'])}\")\n",
    "print(f\"Misclassified: {len(misclassified_idx)}/{len(results['labels'])}\")\n",
    "\n",
    "if len(misclassified_idx) > 0:\n",
    "    print(\"\\nMisclassified samples:\")\n",
    "    for idx in misclassified_idx:\n",
    "        true_label = idx_to_label[results[\"labels\"][idx]]\n",
    "        pred_label = idx_to_label[results[\"predictions\"][idx]]\n",
    "        print(f\"  Sample {idx}: True={true_label}, Predicted={pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26227e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize misclassified spectrograms\n",
    "if len(misclassified_idx) > 0:\n",
    "    n_show = min(len(misclassified_idx), 8)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        idx = misclassified_idx[i]\n",
    "        true_label = idx_to_label[results[\"labels\"][idx]]\n",
    "        pred_label = idx_to_label[results[\"predictions\"][idx]]\n",
    "        \n",
    "        axes[i].imshow(X_test[idx], aspect='auto', origin='lower', cmap='magma')\n",
    "        axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=10)\n",
    "        axes[i].set_xlabel(\"Time\")\n",
    "        axes[i].set_ylabel(\"Mel\")\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_show, 8):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Misclassified Samples\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No misclassified samples!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac8b3d",
   "metadata": {},
   "source": [
    "### Most Confused Class Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most confused pairs (off-diagonal elements of confusion matrix)\n",
    "confusion_pairs = []\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'true': labels[i],\n",
    "                'predicted': labels[j],\n",
    "                'count': cm[i, j]\n",
    "            })\n",
    "\n",
    "# Sort by count\n",
    "confusion_pairs = sorted(confusion_pairs, key=lambda x: -x['count'])\n",
    "\n",
    "print(\"Most Confused Pairs (True ‚Üí Predicted):\")\n",
    "print(\"-\" * 40)\n",
    "for pair in confusion_pairs[:10]:\n",
    "    print(f\"  {pair['true']:12} ‚Üí {pair['predicted']:12}: {pair['count']} times\")\n",
    "\n",
    "if not confusion_pairs:\n",
    "    print(\"  No confusion pairs (perfect classification!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475081a",
   "metadata": {},
   "source": [
    "## 4. Summary & Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70906005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL DIAGNOSTICS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä Cross-Validation (5-fold):\")\n",
    "print(f\"   Accuracy: {cv_results['mean_acc']:.1%} ¬± {cv_results['std_acc']:.1%}\")\n",
    "print(f\"   Best fold: {max(cv_results['accuracies']):.1%}\")\n",
    "print(f\"   Worst fold: {min(cv_results['accuracies']):.1%}\")\n",
    "\n",
    "print(f\"\\nüéØ Test Set Performance:\")\n",
    "print(f\"   Accuracy: {results['accuracy']:.1%}\")\n",
    "print(f\"   Correct: {len(correct_idx)}/{len(results['labels'])}\")\n",
    "print(f\"   Errors: {len(misclassified_idx)}/{len(results['labels'])}\")\n",
    "\n",
    "if confusion_pairs:\n",
    "    print(f\"\\n‚ö†Ô∏è  Most Confused Pair:\")\n",
    "    print(f\"   {confusion_pairs[0]['true']} ‚Üí {confusion_pairs[0]['predicted']}\")\n",
    "\n",
    "# Find best and worst classes\n",
    "best_class = labels[np.argmax(f1)]\n",
    "worst_class = labels[np.argmin(f1)]\n",
    "print(f\"\\n‚úÖ Best performing class: {best_class} (F1: {max(f1):.2f})\")\n",
    "print(f\"‚ùå Worst performing class: {worst_class} (F1: {min(f1):.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
